# -*- coding: utf-8 -*-
"""DSPH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1di78SmOxL7-LF6hzDCcs_Lipzu9XbcY0

### 挂载谷歌云盘并设定程序目录
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.chdir("/content/gdrive/My Drive/Colab Notebooks/Hashing")

"""### 导入需要的包"""

import torch
import torch.optim as optim
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torchvision import models
import numpy as np

"""### 定义一些超参数"""

EPOCHES = 100
BATCH_SIZE = 64
IMAGE_SIZE = 224
LAMBDA = 50
CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if CUDA else "cpu")
IMAGE_MEAN = (0.485, 0.456, 0.406)
IMAGE_STD = (0.229, 0.224, 0.225)
BIT = 12
LEARNING_RATE = 5e-2
WEIGHT_DECAY = 10e-5

"""### 一些方法"""

# 将数字型类别转化为
def EncodingOnehot(target, nclasses):
    target_onehot = torch.FloatTensor(target.size(0), nclasses).to(DEVICE)
    target_onehot.zero_()
    target_onehot.scatter_(1, target.view(-1, 1), 1)
    return target_onehot

# 根据onehot编码计算相似矩阵
def CalcSim(onehot_label):
    S = (onehot_label.mm(onehot_label.t()) > 0).type(torch.FloatTensor).to(DEVICE)
    return S

# 计算log(1 + e ^ Theta(i, j))
def Logtrick(x):
    lt = torch.log(1 + torch.exp(-torch.abs(x))) + torch.max(x, torch.FloatTensor([0.]).to(DEVICE))
    return lt

# 根据epoch自动调整学习率
def AdjustLearningRate(optimizer, epoch, learning_rate):
    lr = learning_rate * (0.1 ** (epoch // 30))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return optimizer

# 生成哈希码
def GenerateCode(model, data_loader, num_data, bit):
    data_ind = 0
    B = np.zeros([num_data, bit], dtype=np.float32)
    for iter, data in enumerate(data_loader, 0):
        x, y = data
        x = x.to(DEVICE)
        output = model(x)
        start = data_ind * BATCH_SIZE
        end = (data_ind + 1) * BATCH_SIZE
        end = end if end <= num_data else num_data
        B[start:end, :] = torch.sign(output.data).cpu().numpy()
        data_ind += 1
    return B

def CalcHammingDist(B1, B2):
    q = B2.shape[1]
    distH = 0.5 * (q - np.dot(B1, B2.transpose()))
    return distH

def CalcMap(qB, rB, queryL, retrievalL):
    # qB: {-1,+1}^{mxq}
    # rB: {-1,+1}^{nxq}
    # queryL: {0,1}^{mxl}
    # retrievalL: {0,1}^{nxl}
    num_query = queryL.shape[0]
    map = 0
    # print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')

    for iter in range(num_query):
        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)
        tsum = np.sum(gnd)
        if tsum == 0:
            continue
        hamm = CalcHammingDist(qB[iter, :], rB)
        ind = np.argsort(hamm)
        gnd = gnd[ind]
        count = np.linspace(1, tsum, tsum)

        tindex = np.asarray(np.where(gnd == 1)) + 1.0
        map_ = np.mean(count / (tindex))
        # print(map_)
        map = map + map_
    map = map / num_query
    # print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')

    return map

"""### 读入数据"""

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(IMAGE_MEAN, IMAGE_STD)
])

train_set = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,
                                          shuffle=True, num_workers=4)
test_set = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,
                                         shuffle=False, num_workers=4)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

nclasses = 10

num_train, num_test = len(train_set), len(test_set)

"""### 定义模型和优化器"""

# 定义模型类
class CNN_Model(nn.Module):
    def __init__(self, bit):
        super(CNN_Model, self).__init__()
        original_model = models.vgg11(pretrained=True)
        self.features = original_model.features
        cl1 = nn.Linear(25088, 4096)
        cl1.weight = original_model.classifier[0].weight
        cl1.bias = original_model.classifier[0].bias

        cl2 = nn.Linear(4096, 4096)
        cl2.weight = original_model.classifier[3].weight
        cl2.bias = original_model.classifier[3].bias

        self.classifier = nn.Sequential(
            cl1,
            nn.ReLU(inplace=True),
            nn.Dropout(),
            cl2,
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, bit),
        )

    def forward(self, x):
        f = self.features(x)
        f = f.view(f.size(0), -1)
        y = self.classifier(f)
        return y

# 实例化模型
model = CNN_Model(BIT).to(DEVICE)

# 这里使用最简单SGD优化器
optimizer = optim.SGD(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY)

"""### 训练阶段"""

for epoch in range(EPOCHES):

    # 记录每个epoch的平均loss
    epoch_loss = 0.0

    for iter, train_data in enumerate(train_loader):
        x, y = train_data
        x = x.to(DEVICE)
        y = y.to(DEVICE)
        y_onehot = EncodingOnehot(y, nclasses)
        # 计算相似矩阵
        s = CalcSim(y_onehot)

        # 梯度清零
        model.zero_grad()

        # x输入模型得到y，即x的BIT个特征，相当于哈希码的12位，只不过不是二进制型的
        u = model(x)

        # 符号化把u转化为{-1， 1}
        b = torch.sign(u)

        # 计算损失
        theta_x = u.mm(u.t()) / 2
        logloss = (s * theta_x - Logtrick(theta_x)).sum() / (len(x) * num_train) # 哈希损失
        regterm = (b - u).pow(2).sum() / (len(x) * num_train)                    # 量化损失
        loss =  - logloss + LAMBDA * regterm

        # 误差反传，优化参数
        loss.backward()
        optimizer.step()

        # 记录损失
        epoch_loss += loss.item()
        print('iter %3d loss: %3.5f' % (iter, loss))

    # 更新学习率
    optimizer = AdjustLearningRate(optimizer, epoch, LEARNING_RATE)
    print('[Train Phase][Epoch: %3d/%3d][Loss: %3.5f]' % (epoch + 1, EPOCHES, epoch_loss / len(train_loader)))
    # 保存整个模型
    torch.save(model, 'model.pkl')

"""### 读取模型"""

model = torch.load('model.pkl')

"""### 测试模型"""

train_label = [data[1] for data in train_set]
test_label = [data[1] for data in test_set]
test_labels_onehot = EncodingOnehot(torch.LongTensor(test_label).to(DEVICE), nclasses)

all_label = train_label + test_label
all_label = torch.LongTensor(all_label).to(DEVICE)
all_labels_onehot = EncodingOnehot(all_label, nclasses)

train_B = GenerateCode(model, train_loader, num_train, BIT)
test_B = GenerateCode(model, test_loader, num_test, BIT)
data_B = np.concatenate((train_B, test_B), axis=0)

map = CalcMap(test_B, data_B, test_labels_onehot.cpu().numpy(), all_labels_onehot.cpu().numpy())

map
